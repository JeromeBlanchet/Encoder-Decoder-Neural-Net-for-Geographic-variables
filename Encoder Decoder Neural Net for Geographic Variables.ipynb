{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revised version of Kutsal Baran Özkurt's program.\n",
    "# Özkurt program propose a 3 layer encoder-decoder neural network. \n",
    "# Input layer is a geography variable including a large number of categories (thousands of categories)\n",
    "# Input layer get's information from a binary (sparse dataset representation) of the original categorical variable.\n",
    "# Output layer is a relatively more aggregated geography variable, but still include a large number of categories.\n",
    "# Intuitivelly, it make sense to use the more granular geography variable as input, and the aggregated one as output.\n",
    "# Note: Output layer is in fact, 2 variable. So, the neural network is learning to minimize in 2 different ways.\n",
    "# The neural network encode the information, and decode it back to the aggregate variable.\n",
    "# Mid layer is the encoded vector, and include 16 neurons.\n",
    "# Considering the input and output layer have very large number of neurons. \n",
    "# A mid layer with 16 neurons can be seen as a way to compress the information. that is, dimensionality reduction \n",
    "# A competitive way to do it would be with PCA, t-SNE or uMAP.\n",
    "# The respective single output of these 16 neurons will be stored as 16 new engineered variables in the original dataset.\n",
    "# Mid layer output propose a more dense information than traditional bynary encoding (no sparse dataset issue)\n",
    "# The 3 original geography variable are then excluded from the final dataset.\n",
    "# The dense 16-dimension engineered vector, not only get ride of the granular categorical variables, \n",
    "# but also summarize the relationship between these 3 variables.\n",
    "\n",
    "# Original code includes issue regarding the k.function for mid layer data extraction.\n",
    "# I propose a different notation (more efficient slicing of the object) that work at once.\n",
    "# Other improvement include using a 20-fold cross validation instead of 5 fold. \n",
    "# More fold equal a model learning on more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My version with proposed improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import keras \n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "SEED = 1881\n",
    "\n",
    "train_x = pd.read_csv(\"../input/richters-predictor-modeling-earthquake-damage/train_values.csv\")\n",
    "train_y = pd.read_csv(\"../input/richters-predictor-modeling-earthquake-damage/train_labels.csv\")\n",
    "test_x  = pd.read_csv(\"../input/richters-predictor-modeling-earthquake-damage/test_values.csv\")\n",
    "sub_csv = pd.read_csv(\"../input/richters-predictor-modeling-earthquake-damage/submission_format.csv\")\n",
    "\n",
    "geo1 = np.array(pd.get_dummies(pd.concat([train_x[\"geo_level_1_id\"], test_x[\"geo_level_1_id\"]])))\n",
    "geo2 = np.array(pd.get_dummies(pd.concat([train_x[\"geo_level_2_id\"], test_x[\"geo_level_2_id\"]])))\n",
    "geo3 = np.array(pd.get_dummies(pd.concat([train_x[\"geo_level_3_id\"], test_x[\"geo_level_3_id\"]])))\n",
    "\n",
    "def NET():\n",
    "    inp = Input((geo3.shape[1],))\n",
    "    i1 = Dense(16, name=\"intermediate\")(inp)\n",
    "    x2 = Dense(geo2.shape[1], activation='sigmoid')(i1)\n",
    "    x1 = Dense(geo1.shape[1], activation='sigmoid')(i1)\n",
    "    model = Model(inp, [x2,x1])\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "    return model\n",
    "\n",
    "model = NET()\n",
    "model.fit(geo3, [geo2, geo1], batch_size=128, epochs=10, verbose=2)\n",
    "\n",
    "model.save(\"geo_embed.h5\")\n",
    "model.load_weights(\"geo_embed.h5\")\n",
    "\n",
    "get_int_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[1].output])\n",
    "\n",
    "out = []\n",
    "\n",
    "#------------------------Proposed new notation--------------------------------------\n",
    "\n",
    "for dat in range(260601):\n",
    "    layer_output = get_int_layer_output([[geo3[dat:dat+1,:]]])[0]\n",
    "    out.append(layer_output)\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "    \n",
    "out = np.array(out)\n",
    "out = np.squeeze(out)\n",
    "\n",
    "train_data = pd.get_dummies(train_x.copy())\n",
    "train_data = train_data.drop(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'], axis=1)\n",
    "train_data = train_data.assign(geo_feat1=out[:,0],\n",
    "                               geo_feat2=out[:,1],\n",
    "                               geo_feat3=out[:,2],  \n",
    "                               geo_feat4=out[:,3],\n",
    "                               geo_feat5=out[:,4],    \n",
    "                               geo_feat6=out[:,5],\n",
    "                               geo_feat7=out[:,6],\n",
    "                               geo_feat8=out[:,7],\n",
    "                               geo_feat9=out[:,8],\n",
    "                               geo_feat10=out[:,9],\n",
    "                               geo_feat11=out[:,10],\n",
    "                               geo_feat12=out[:,11],\n",
    "                               geo_feat13=out[:,12],\n",
    "                               geo_feat14=out[:,13],\n",
    "                               geo_feat15=out[:,14],           \n",
    "                               geo_feat16=out[:,15])\n",
    "\n",
    "out = []\n",
    "\n",
    "#------------------------Proposed new notation--------------------------------------\n",
    "\n",
    "for dat in range(260601, 347469):\n",
    "    layer_output = get_int_layer_output([[geo3[dat:dat+1,:]]])[0]\n",
    "    out.append(layer_output)\n",
    "    \n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "out = np.array(out)\n",
    "out = np.squeeze(out)\n",
    "\n",
    "test_data = pd.get_dummies(test_x.copy())\n",
    "test_data = test_data.drop(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'], axis=1)\n",
    "test_data = test_data.assign(geo_feat1=out[:,0],\n",
    "                             geo_feat2=out[:,1],\n",
    "                             geo_feat3=out[:,2],  \n",
    "                             geo_feat4=out[:,3],\n",
    "                             geo_feat5=out[:,4],    \n",
    "                             geo_feat6=out[:,5],\n",
    "                             geo_feat7=out[:,6],\n",
    "                             geo_feat8=out[:,7],\n",
    "                             geo_feat9=out[:,8],\n",
    "                             geo_feat10=out[:,9],\n",
    "                             geo_feat11=out[:,10],\n",
    "                             geo_feat12=out[:,11],\n",
    "                             geo_feat13=out[:,12],\n",
    "                             geo_feat14=out[:,13],\n",
    "                             geo_feat15=out[:,14],           \n",
    "                             geo_feat16=out[:,15])\n",
    "\n",
    "def threshold_arr(array):\n",
    "    new_arr = []\n",
    "    for ix, val in enumerate(array):\n",
    "        loc = np.array(val).argmax(axis=0)\n",
    "        k = list(np.zeros((len(val))))\n",
    "        k[loc]=1\n",
    "        new_arr.append(k)\n",
    "        \n",
    "    return np.array(new_arr)\n",
    "\n",
    "#------------------------Proposed new notation--------------------------------------\n",
    "\n",
    "y = np.array(train_y[\"damage_grade\"])-1\n",
    "df = train_data.drop([\"building_id\"], axis=1)\n",
    "x = np.array(df)\n",
    "kf = KFold(n_splits=20, shuffle=True, random_state=SEED)\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "for ix, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "    lgb_params = {\n",
    "        \"objective\" : \"multiclass\",\n",
    "        \"num_class\":3,\n",
    "        \"metric\" : \"multi_error\",\n",
    "        \"boosting\": 'gbdt',\n",
    "        \"max_depth\" : -1,\n",
    "        \"num_leaves\" : 30,\n",
    "        \"learning_rate\" : 0.1,\n",
    "        \"feature_fraction\" : 0.5,\n",
    "        \"min_sum_hessian_in_leaf\" : 0.1,\n",
    "        \"max_bin\":8192,\n",
    "        \"verbosity\" : 1,\n",
    "        \"num_threads\":6,\n",
    "        \"seed\": SEED\n",
    "    }\n",
    "\n",
    "    x_train, x_val, y_train, y_val= x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "\n",
    "    train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    val_data   = lgb.Dataset(x_val, label=y_val)\n",
    "\n",
    "    lgb_clf = lgb.train(lgb_params,\n",
    "                        train_data,\n",
    "                        20000,\n",
    "                        valid_sets = [val_data],\n",
    "                        early_stopping_rounds=3000,\n",
    "                        verbose_eval = 1000)\n",
    "\n",
    "    y_pred = lgb_clf.predict(x_val)\n",
    "    print(\"F1-MICRO SCORE: \", f1_score(np.array(pd.get_dummies(y_val)), threshold_arr(y_pred), average='micro'))\n",
    "    lgb_clf.save_model(f'../working/model{ix}.txt')\n",
    "    \n",
    "models = []\n",
    "\n",
    "for i in range(20):\n",
    "    model = lgb.Booster(model_file=f'../working/model{i}.txt')\n",
    "    y_pred = model.predict(x)\n",
    "    score  = f1_score(np.array(pd.get_dummies(y)), threshold_arr(y_pred), average='micro')\n",
    "    print(\"F1-MICRO SCORE: \", score)\n",
    "    models.append(model)\n",
    "    \n",
    "def ensemble(models, x):\n",
    "    y_preds = []\n",
    "    \n",
    "    for model in models:\n",
    "        y_pred = model.predict(x)\n",
    "        y_preds.append(y_pred)\n",
    "        \n",
    "    init_y_pred = y_preds[0]\n",
    "    \n",
    "    for ypred in y_preds[1:]:\n",
    "        init_y_pred += ypred\n",
    "        \n",
    "    y_pred = threshold_arr(init_y_pred)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "df = test_data.drop([\"building_id\"], axis=1)\n",
    "x = np.array(df)\n",
    "y_pred = ensemble(models, x)\n",
    "y_pred = y_pred.argmax(axis=1)+1\n",
    "sub_csv[\"damage_grade\"] = y_pred\n",
    "sub_csv.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annexe - Original Code from Kutsal Baran Özkurt \n",
    "# can be found here https://github.com/Goodsea/Richter-s-Eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import keras \n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "DIR  = \"data/\"\n",
    "SEED = 1881\n",
    "\n",
    "if not os.path.isdir(\"models/\"):\n",
    "    os.makedirs(\"models\")\n",
    "    \n",
    "print(os.listdir(DIR))\n",
    "\n",
    "train_x = pd.read_csv(DIR+\"train_values.csv\")\n",
    "train_y = pd.read_csv(DIR+\"train_labels.csv\")\n",
    "test_x  = pd.read_csv(DIR+\"test_values.csv\")\n",
    "sub_csv = pd.read_csv(DIR+\"submission_format.csv\")\n",
    "\n",
    "geo1 = np.array(pd.get_dummies(pd.concat([train_x[\"geo_level_1_id\"], test_x[\"geo_level_1_id\"]])))\n",
    "geo2 = np.array(pd.get_dummies(pd.concat([train_x[\"geo_level_2_id\"], test_x[\"geo_level_2_id\"]])))\n",
    "geo3 = np.array(pd.get_dummies(pd.concat([train_x[\"geo_level_3_id\"], test_x[\"geo_level_3_id\"]])))\n",
    "\n",
    "geo3.shape\n",
    "\n",
    "def NET():\n",
    "    inp = Input((geo3.shape[1],))\n",
    "    i1 = Dense(16, name=\"intermediate\")(inp)\n",
    "    x2 = Dense(geo2.shape[1], activation='sigmoid')(i1)\n",
    "    x1 = Dense(geo1.shape[1], activation='sigmoid')(i1)\n",
    "\n",
    "    model = Model(inp, [x2,x1])\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "    return model\n",
    "\n",
    "model = NET()\n",
    "model.fit(geo3, [geo2, geo1], batch_size=128, epochs=10, verbose=2)\n",
    "model.save(\"geo_embed.h5\")\n",
    "\n",
    "model = NET()\n",
    "model.load_weights(\"geo_embed.h5\")\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "get_int_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[1].output])\n",
    "\n",
    "out = []\n",
    "for dat in geo3[:260601]:\n",
    "    layer_output = get_int_layer_output([[dat]])[0]\n",
    "    out.append(layer_output)\n",
    "\n",
    "out = np.array(out)\n",
    "out = np.squeeze(out)\n",
    "\n",
    "train_data = pd.get_dummies(train_x.copy())\n",
    "train_data = train_data.drop(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'], axis=1)\n",
    "train_data = train_data.assign(geo_feat1=out[:,0],\n",
    "                               geo_feat2=out[:,1],\n",
    "                               geo_feat3=out[:,2],  \n",
    "                               geo_feat4=out[:,3],\n",
    "                               geo_feat5=out[:,4],    \n",
    "                               geo_feat6=out[:,5],\n",
    "                               geo_feat7=out[:,6],\n",
    "                               geo_feat8=out[:,7],\n",
    "                               geo_feat9=out[:,8],\n",
    "                               geo_feat10=out[:,9],\n",
    "                               geo_feat11=out[:,10],\n",
    "                               geo_feat12=out[:,11],\n",
    "                               geo_feat13=out[:,12],\n",
    "                               geo_feat14=out[:,13],\n",
    "                               geo_feat15=out[:,14],           \n",
    "                               geo_feat16=out[:,15])\n",
    "\n",
    "train_data.head()\n",
    "\n",
    "train_data.columns\n",
    "\n",
    "out = []\n",
    "for dat in geo3[260601:]:\n",
    "    layer_output = get_int_layer_output([[dat]])[0]\n",
    "    out.append(layer_output)\n",
    "\n",
    "out = np.array(out)\n",
    "out = np.squeeze(out)\n",
    "\n",
    "test_data = pd.get_dummies(test_x.copy())\n",
    "test_data = test_data.drop(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'], axis=1)\n",
    "test_data = test_data.assign(geo_feat1=out[:,0],\n",
    "                             geo_feat2=out[:,1],\n",
    "                             geo_feat3=out[:,2],  \n",
    "                             geo_feat4=out[:,3],\n",
    "                             geo_feat5=out[:,4],    \n",
    "                             geo_feat6=out[:,5],\n",
    "                             geo_feat7=out[:,6],\n",
    "                             geo_feat8=out[:,7],\n",
    "                             geo_feat9=out[:,8],\n",
    "                             geo_feat10=out[:,9],\n",
    "                             geo_feat11=out[:,10],\n",
    "                             geo_feat12=out[:,11],\n",
    "                             geo_feat13=out[:,12],\n",
    "                             geo_feat14=out[:,13],\n",
    "                             geo_feat15=out[:,14],           \n",
    "                             geo_feat16=out[:,15])\n",
    "\n",
    "test_data.head()\n",
    "\n",
    "test_data.columns\n",
    "\n",
    "def threshold_arr(array):\n",
    "    # Get major confidence-scored predicted value.\n",
    "    new_arr = []\n",
    "    for ix, val in enumerate(array):\n",
    "        loc = np.array(val).argmax(axis=0)\n",
    "        k = list(np.zeros((len(val))))\n",
    "        k[loc]=1\n",
    "        new_arr.append(k)\n",
    "        \n",
    "    return np.array(new_arr)\n",
    "\n",
    "y = np.array(train_y[\"damage_grade\"])-1\n",
    "\n",
    "df = train_data.drop([\"building_id\"], axis=1)\n",
    "x = np.array(df)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "for ix, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "    lgb_params = {\n",
    "        \"objective\" : \"multiclass\",\n",
    "        \"num_class\":3,\n",
    "        \"metric\" : \"multi_error\",\n",
    "        \"boosting\": 'gbdt',\n",
    "        \"max_depth\" : -1,\n",
    "        \"num_leaves\" : 30,\n",
    "        \"learning_rate\" : 0.1,\n",
    "        \"feature_fraction\" : 0.5,\n",
    "        \"min_sum_hessian_in_leaf\" : 0.1,\n",
    "        \"max_bin\":8192,\n",
    "        \"verbosity\" : 1,\n",
    "        \"num_threads\":6,\n",
    "        \"seed\": SEED\n",
    "    }\n",
    "\n",
    "    x_train, x_val, y_train, y_val= x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "\n",
    "    train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    val_data   = lgb.Dataset(x_val, label=y_val)\n",
    "\n",
    "    lgb_clf = lgb.train(lgb_params,\n",
    "                        train_data,\n",
    "                        20000,\n",
    "                        valid_sets = [val_data],\n",
    "                        early_stopping_rounds=3000,\n",
    "                        verbose_eval = 1000)\n",
    "\n",
    "    y_pred = lgb_clf.predict(x_val)\n",
    "    print(\"F1-MICRO SCORE: \", f1_score(np.array(pd.get_dummies(y_val)), threshold_arr(y_pred), average='micro'))\n",
    "    lgb_clf.save_model(f'models/model{ix}.txt')\n",
    "    \n",
    "models = []\n",
    "for i in range(5):\n",
    "    model = lgb.Booster(model_file=f'models/model{i}.txt')\n",
    "\n",
    "    y_pred = model.predict(x)\n",
    "    score  = f1_score(np.array(pd.get_dummies(y)), threshold_arr(y_pred), average='micro')\n",
    "    print(\"F1-MICRO SCORE: \", score)\n",
    "    models.append(model)\n",
    "    \n",
    "    # Ensemble K-Fold CV models with adding all confidence score by class.\n",
    "    y_preds = []\n",
    "    \n",
    "    for model in models:\n",
    "        y_pred = model.predict(x)\n",
    "        y_preds.append(y_pred)\n",
    "        \n",
    "    init_y_pred = y_preds[0]\n",
    "    for ypred in y_preds[1:]:\n",
    "        init_y_pred += ypred\n",
    "        \n",
    "    y_pred = threshold_arr(init_y_pred)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "df = test_data.drop([\"building_id\"], axis=1)\n",
    "x = np.array(df)\n",
    "\n",
    "y_pred = ensemble(models, x)\n",
    "y_pred = y_pred.argmax(axis=1)+1\n",
    "\n",
    "sub_csv[\"damage_grade\"] = y_pred\n",
    "sub_csv.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
